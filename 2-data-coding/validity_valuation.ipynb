{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Validity of the Valuation Exclusion\n",
    "\n",
    "During the data extraction and coding step, we excluded attributes which *do not imply valuation*, i.e., where the values of the attribute do not inform about the quality of the activity. To assess the validity of this step, a second researcher performed the same task independently. This notebook calculates the agreement between the two raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name: str = './r3a-data-extraction.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percentage_agreement(rating1: list, rating2: list) -> float:\n",
    "    \"\"\"Calculate the percentage agreement between two ordered lists of ratings.\n",
    "\n",
    "    parameters:\n",
    "        rating1 - list of values of one rater\n",
    "        rating2 - list of values of another rater\n",
    "\n",
    "    returns\n",
    "        percentage agreement - the fraction of perfect matches in the two ratings between 0 and 1\n",
    "        ValueError - raised if the two lists are not of equal length\n",
    "    \"\"\"\n",
    "\n",
    "    if len(rating1) != len(rating2):\n",
    "        raise ValueError(f'The two ratings to compare are of inequal length ({len(rating1)} vs. {len(rating1)}).')\n",
    "    \n",
    "    n: int = len(rating1)\n",
    "    agreement: float = 0\n",
    "    for (r1, r2) in zip(rating1, rating2):\n",
    "        if r1 == r2:\n",
    "            agreement += 1\n",
    "\n",
    "    return agreement/n\n",
    "\n",
    "def calc_bennetts_s_score(rating1: list, rating2: list, labels: list) -> float:\n",
    "    \"\"\"Calculate the Bennett's S score, which represents the agreement between two rater over two ratings given a set of possible labels.\n",
    "    \n",
    "    parameters:\n",
    "        rating1 - list of values of one rater\n",
    "        rating2 - list of values of another rater\n",
    "        labels - list of potential values\n",
    "\n",
    "    returns\n",
    "        s score - Bennett's S score of agreement between 0 and 1\n",
    "        ValueError - raised if the two lists are not of equal length\n",
    "    \"\"\"\n",
    "    k = len(labels)\n",
    "    p = calc_percentage_agreement(rating1, rating2)\n",
    "    return (k/(k-1)) * (p-(1/k))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison 1\n",
    "\n",
    "### Data Loading\n",
    "\n",
    "Firstly, we load the data from the excel sheet of extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating1 = pd.read_excel(file_name, sheet_name='Data', usecols=['ID', 'Attribute Description', 'Val'])\n",
    "rating2 = pd.read_excel(file_name, sheet_name='Valuation Overlap v1', usecols=['ID', 'Dependent Variable', 'No valuation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we filter the ratings of the first researcher to contain only those that were also rated by the second researcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_ids = rating2['ID'].values\n",
    "relevant_dependent_variables = rating2['Dependent Variable'].values\n",
    "rating1_relevant = rating1[(rating1['ID'].isin(relevant_ids)) & \n",
    "                           (rating1['Attribute Description'].isin(relevant_dependent_variables))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating1_relevant = rating1_relevant.drop_duplicates().set_index('ID')\n",
    "rating2 = rating2.set_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the two data frames to obtain one data frame containing both ratings, now named `R1` and `R2` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating1_relevant.rename(columns={'Val': 'R1'}, inplace=True)\n",
    "rating2.drop(columns='Dependent Variable', inplace=True)\n",
    "rating2.rename(columns={'No valuation': 'R2'}, inplace=True)\n",
    "overlap1 = pd.concat([rating1_relevant, rating2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Agreement\n",
    "\n",
    "Next, we calculate the agreement of the ratings of both researchers by the means of percentage agreement[1], Cohen's Kappa [2], and Bennett's S-score [3].\n",
    "\n",
    "Percentage agreement is the simplest type of inter-rater reliability. It suffers from the fact that it does not account for agreement caused by chance. Cohen's Kappa accounts for agreement caused by chance but samples the expected marginal distributions from the data directly. Bennett's S-score is a recommended alternative to Cohen's Kappa since it does account for agreement caused by chance but does assume an even marginal distribution. We report all three measures for completeness sake.\n",
    "\n",
    "[1] Holsti, O. R. (1969). Content analysis for the social sciences and humanities. Reading. MA: Addison-Wesley (content analysis).\n",
    "[2] Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1), 37-46.\n",
    "[3] Bennett, E. M., Alpert, R., & Goldstein, A. C. (1954). Communications through limited-response questioning. Public Opinion Quarterly, 18(3), 303-308."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two raters achieved a percentage agreement of 66.67%, a Cohen's Kappa agreement of 33.33%, and a Bennett's S-Score of 33.33%.\n"
     ]
    }
   ],
   "source": [
    "percentage_agreement = calc_percentage_agreement(overlap1['R1'], overlap1['R2'])\n",
    "cohens_kappa = cohen_kappa_score(overlap1['R1'], overlap1['R2'], labels=[True, False])\n",
    "s_score = calc_bennetts_s_score(overlap1['R1'], overlap1['R2'], labels=[True, False])\n",
    "\n",
    "print(f\"The two raters achieved a percentage agreement of {percentage_agreement:.2%}, a Cohen's Kappa agreement of {cohens_kappa:.2%}, and a Bennett's S-Score of {s_score:.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison 2\n",
    "\n",
    "The Cohen's Kappa agreement of 33.33% is very poor. The two researchers, hence, discussed the rating criteria once more and performed the labeling task again.\n",
    "\n",
    "### Data Loading\n",
    "\n",
    "The results of the second rating are stored in the sheet `Valuation Overlap v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rating3 = pd.read_excel(file_name, sheet_name='Valuation Overlap v2', usecols=['ID', 'No valuation']).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating3.rename(columns={'No valuation': 'R2'}, inplace=True)\n",
    "overlap2 = pd.concat([rating1_relevant, rating3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Agreement\n",
    "\n",
    "Again, we calculate the agreement of the two raters using the three metrics [1,2,3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two raters achieved a percentage agreement of 91.67%, a Cohen's Kappa agreement of 83.33%, and a Bennett's S-Score of 83.33%.\n"
     ]
    }
   ],
   "source": [
    "percentage_agreement = calc_percentage_agreement(overlap2['R1'], overlap2['R2'])\n",
    "cohens_kappa = cohen_kappa_score(overlap2['R1'], overlap2['R2'], labels=[True, False])\n",
    "s_score = calc_bennetts_s_score(overlap2['R1'], overlap2['R2'], labels=[True, False])\n",
    "\n",
    "print(f\"The two raters achieved a percentage agreement of {percentage_agreement:.2%}, a Cohen's Kappa agreement of {cohens_kappa:.2%}, and a Bennett's S-Score of {s_score:.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage agreement is high, but unreliable in general. The Cohen's Kappa score and Bennet's S-score are equal because of the even marginal distributions. Both values are sufficiently high to validate a common understanding of the subjective task, now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
