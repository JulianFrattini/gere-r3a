{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Evaluation\n",
    "\n",
    "This notebook generates descriptive statistics about the data and its cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./r3a-data-extraction.xlsx', sheet_name='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_codes(df: pd.DataFrame, column: str) -> [str]:\n",
    "    \"\"\"Returns a list of unique codes contained in a given column.\n",
    "\n",
    "    parameters:\n",
    "        df -- pandas.DataFrame containing the data\n",
    "        column -- name of the column containing cells with codes or groups of codes separated by semicolons (e.g., \"code1;code2)\n",
    "\n",
    "    returns:\n",
    "        list of unique codes\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain all codes of the column (which still contains code groups like \"code1;code2\")\n",
    "    all_codes = list(df[column].value_counts().index)\n",
    "\n",
    "    # split up code groups (the sum([...], []) flattens the list of lists)\n",
    "    singular_codes = sum([code.split(';') for code in all_codes], [])\n",
    "\n",
    "    # remove duplicates\n",
    "    return list(set(singular_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Removing wrongfully included Experimentation Literature\n",
    "\n",
    "The inclusion and exclusion criteria were applied to the title, abstract, and keywords of the 1446 primary studies retrieved by the database search. In some cases, however, the data extraction revealed that the assumptions, under which a study was included, did not hold. These studies need to be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 primary studies were wrongfully included and have to be removed.\n"
     ]
    }
   ],
   "source": [
    "# identify the IDs of the wrongfully included papers, which have a True flag in the 'F' column\n",
    "ids_of_wrongfully_included = set(df[(df['Type']=='E') & (df['F']==True)]['ID'].values)\n",
    "print(f'{len(ids_of_wrongfully_included)} primary studies were wrongfully included and have to be removed.')\n",
    "\n",
    "# remove all these wrongfully included primary studies (i.e., false positives) from the data set\n",
    "df = df[df['F']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-valuating Attributes\n",
    "\n",
    "Several dependent variables in the data set describe non-valuating attributes, i.e., properties of the activity that do not have a connection to quality. These need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 studies use non-valuating dependent variables that are irrelevant to this study.\n"
     ]
    }
   ],
   "source": [
    "# identify the IDs of the non-valuating dependent variables, which have a True flag in the 'F' column\n",
    "ids_of_nonvaluating = set(df[(df['Type']=='E') & (df['Val']==True)])\n",
    "print(f'{len(ids_of_nonvaluating)} studies use non-valuating dependent variables that are irrelevant to this study.')\n",
    "\n",
    "# remove all these wrongfully included data points (i.e., false positives) from the data set\n",
    "df = df[df['Val']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluation\n",
    "\n",
    "In the following code blocks, we evaluate the cleaned data and generate some general, high-level statistics.\n",
    "\n",
    "### Number of textual Descriptions\n",
    "\n",
    "Firstly, we count the number of textual descriptions extracted for each data source type and in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Experimental Literature contained 142 descriptions of activities and 355 descriptions of attributes.\n",
      "The Interview Study contained 55 descriptions of activities and 0 descriptions of attributes.\n",
      "The Software Process Literature contained 21 descriptions of activities and 1 descriptions of attributes.\n"
     ]
    }
   ],
   "source": [
    "data_source_types = {\n",
    "    'E': 'Experimental Literature',\n",
    "    'I': 'Interview Study', \n",
    "    'S': 'Software Process Literature'\n",
    "}\n",
    "\n",
    "for dst in data_source_types:\n",
    "    df_specific = df[df['Type'] == dst]\n",
    "    n_activity_mentions = len(df_specific[df_specific['Activity Description'].notnull()])\n",
    "    n_attribute_mentions = len(df_specific[df_specific['Attribute Description'].notnull()])\n",
    "    print(f'The {data_source_types[dst]} contained {n_activity_mentions} descriptions of activities and {n_attribute_mentions} descriptions of attributes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete data set of extractions contained 218 descriptions of activities and 356 descriptions of attributes.\n"
     ]
    }
   ],
   "source": [
    "n_activity_mentions = len(df[df['Activity Description'].notnull()])\n",
    "n_attribute_mentions = len(df[df['Attribute Description'].notnull()])\n",
    "print(f'The complete data set of extractions contained {n_activity_mentions} descriptions of activities and {n_attribute_mentions} descriptions of attributes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique Codes\n",
    "\n",
    "Next, we determine the number of unique codes per the four categories: activity, activity attribute, artifact, and artifact attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set contains 24 unique activities, 16 unique activity-attributes, 21 unique artifacts, and 26 unique artifact-attributes.\n"
     ]
    }
   ],
   "source": [
    "activities = get_unique_codes(df, 'Activity')\n",
    "activity_attributes = get_unique_codes(df, 'Activity Attributes')\n",
    "artifacts = get_unique_codes(df, 'Artifact')\n",
    "artifact_attributes = get_unique_codes(df, 'Artifact Attributes')\n",
    "\n",
    "print(f'The data set contains {len(activities)} unique activities, {len(activity_attributes)} unique activity-attributes, {len(artifacts)} unique artifacts, and {len(artifact_attributes)} unique artifact-attributes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
